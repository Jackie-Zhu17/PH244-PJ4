{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 80\n",
    "in_channels = 30\n",
    "num_classes = 600\n",
    "batch_size = 500\n",
    "learning_rate = 0.00003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "#np.save('X_train.npy', X_train)\n",
    "X_train_load = np.load('X_train.npy')\n",
    "\n",
    "#np.save('y_train.npy', y_train)\n",
    "y_train_load = np.load('y_train.npy')\n",
    "\n",
    "#np.save('X_test.npy', X_test)\n",
    "X_test_load = np.load('X_test.npy')\n",
    "\n",
    "#np.save('y_test.npy', y_test)\n",
    "y_test_load = np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 64, 64, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_load.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedDataset(data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features.swapaxes(1,3)\n",
    "        \n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.features[index] , self.labels[index]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "train_data = MedDataset(X_train_load, y_train_load)\n",
    "test_data = MedDataset(X_test_load, y_test_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset = train_data,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_data,\n",
    "                                          batch_size = 100,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride = 1):\n",
    "    '''\n",
    "    3*3的卷积核，是ResNet常用的卷积核size\n",
    "    '''\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                    stride = stride, padding = 1, bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    '''\n",
    "    这个是构建一个残差块，残差块由（卷积层-规范化层-relu-卷积层-规范化层）组成，其中residual也就是残差，而out是恒同映射的块。\n",
    "    对于残差，其实和普通的CNN构造差不多，但是重点是他拟合的是残差而不是target本身，所以学习的函数是y=(F(x)-y)+y，体现在code中的：\n",
    "    **out += residual**\n",
    "    恒同映射：F(x)=x\n",
    "    我们训练的目标：希望学习到一个H，使得H(x)=x，但由于太困难，于是学习H(x)=F(x)+x，其中F(x)就是残差，残差的恒同映射比起原始参数会更容易学习。\n",
    "    比如：target是学习H(5)=5.5，现在变成了学习H(5)=F(5)+5，那么也即是学习F(5)=0.5，比起F(5)=5.5，这个更容易学习\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample = None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    '''\n",
    "    这是一个残差网络，为了克服传统网络在层数过多时发生的各种问题，比如梯度弥散或者梯度消失等问题，层数过多不但没有提升准确率还会对准确率有所影响。所以为了使得层数\n",
    "    能够尽可能多，尽可能地拟合，需要改变传统CNN的思想。类似于boosting算法，比如gradient boosting tree，每一颗树的损失函数是前一颗树的负梯度（也即是残差），通过\n",
    "    不断拟合残差，能够使得残差越来越小，最终逼近结果；残差网络也是这样的思想，传统的CNN每一个小块是（卷积层，池化层）不断提取特征，ResNet其实结构也差不多。\n",
    "    ResNet每一层有很多个block（这里每一层是两个block），而每个block有两个部分：残差拟合和恒同映射，block的结构构造参考上面的cell：ResidualBlock。\n",
    "    \n",
    "    看完上面ResidualBlock的代码理解后，就知道ResNet是怎么搭建的：\n",
    "    1.首先定义好每个ResidualBlock的结构：残差的拟合和恒同映射！\n",
    "    2.再搭建整个网络：\n",
    "        首先是普通的CNN结构，变量输入，经过一个3*3卷积核，规范化层和激活层，得到隐含的变量；\n",
    "        然后就是搭建每一层（这里是6层），每一层中有layers[i]个block（看后面是每层有2个block）；每个block都是为了拟合上一个block出来的残差：\n",
    "        比如：还是上文的例子，target:H(5)=5.5，学习H(x)=F(x)+x\n",
    "            block1:F1(5)=0.1，那么H1(5)=F1(5)+5=5.1，所以拟合的残差是5.5-5.1=0.4\n",
    "            block2:F2(5)=0.2，那么H2(5)=F2(5)+H1(5)=5.3，所以拟合的残差是5.5-5.3=0.2\n",
    "            block3:F3(5)=0.2，那么H3(5)=F3(5)+23(5)=5.5，与目标H(5)=5.5一致\n",
    "        然后就一直堆层数，由于每次拟合的是残差，所以可能每次会有些许波动，但是这个残差最终肯定是收敛于0的，于是可以一直堆层数\n",
    "    '''\n",
    "    def __init__(self, in_channels, block, layers, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.conv = conv3x3(30, 30)\n",
    "        self.bn = nn.BatchNorm2d(30) \n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.layer1 = self.make_layer(block, 30, 64, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 64, 64, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, 128, layers[2], 2)\n",
    "        self.layer4 = self.make_layer(block, 128, 256, layers[3], 2)\n",
    "        self.layer5 = self.make_layer(block, 256, 512, layers[4], 2)\n",
    "        self.layer6 = self.make_layer(block, 512, 512, layers[5], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        #self.conv2 = conv3x3(64,32)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def make_layer(self, block, in_channels, out_channels, blocks, stride = 1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "            conv3x3(in_channels, out_channels, stride = stride),\n",
    "            nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(in_channels, out_channels, stride, downsample))\n",
    "        in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "            return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "def weight_init(m):\n",
    "    '''\n",
    "    这个是模型的参数初始化，这个应该不是ResNet的部分，这是通用的，就不同的操作对应不同的参数初始化，比如一维卷积是正态随机，2维卷积就是xavier正态随机了\n",
    "    '''\n",
    "    '''\n",
    "    Usage:\n",
    "        model = Model()\n",
    "        model.apply(weight_init)\n",
    "    '''\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm3d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.LSTMCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.GRU):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.GRUCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(in_channels, ResidualBlock, [2,2,2,2,2,2], num_classes).to(device)\n",
    "#model.apply(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Step [1/1], Loss: 110.4457505\n",
      "Epoch [2/80], Step [1/1], Loss: 91.1557292\n",
      "Epoch [3/80], Step [1/1], Loss: 36.7564157\n",
      "Epoch [4/80], Step [1/1], Loss: 3.2678833\n",
      "Epoch [5/80], Step [1/1], Loss: 1.3855586\n",
      "Epoch [6/80], Step [1/1], Loss: 1.1456214\n",
      "Epoch [7/80], Step [1/1], Loss: 1.0826960\n",
      "Epoch [8/80], Step [1/1], Loss: 1.0601135\n",
      "Epoch [9/80], Step [1/1], Loss: 1.0505747\n",
      "Epoch [10/80], Step [1/1], Loss: 1.0466340\n",
      "Epoch [11/80], Step [1/1], Loss: 1.0435207\n",
      "Epoch [12/80], Step [1/1], Loss: 1.0405373\n",
      "Epoch [13/80], Step [1/1], Loss: 1.0381652\n",
      "Epoch [14/80], Step [1/1], Loss: 1.0360132\n",
      "Epoch [15/80], Step [1/1], Loss: 1.0331344\n",
      "Epoch [16/80], Step [1/1], Loss: 1.0294889\n",
      "Epoch [17/80], Step [1/1], Loss: 1.0983928\n",
      "Epoch [18/80], Step [1/1], Loss: 1.0299992\n",
      "Epoch [19/80], Step [1/1], Loss: 1.0282264\n",
      "Epoch [20/80], Step [1/1], Loss: 1.0267476\n",
      "Epoch [21/80], Step [1/1], Loss: 1.0259786\n",
      "Epoch [22/80], Step [1/1], Loss: 1.0248241\n",
      "Epoch [23/80], Step [1/1], Loss: 1.0234937\n",
      "Epoch [24/80], Step [1/1], Loss: 1.0221582\n",
      "Epoch [25/80], Step [1/1], Loss: 1.0208261\n",
      "Epoch [26/80], Step [1/1], Loss: 1.0195688\n",
      "Epoch [27/80], Step [1/1], Loss: 1.0184628\n",
      "Epoch [28/80], Step [1/1], Loss: 1.0176764\n",
      "Epoch [29/80], Step [1/1], Loss: 1.0163936\n",
      "Epoch [30/80], Step [1/1], Loss: 1.0154984\n",
      "Epoch [31/80], Step [1/1], Loss: 1.0136683\n",
      "Epoch [32/80], Step [1/1], Loss: 1.0119675\n",
      "Epoch [33/80], Step [1/1], Loss: 1.0118116\n",
      "Epoch [34/80], Step [1/1], Loss: 1.0085639\n",
      "Epoch [35/80], Step [1/1], Loss: 1.0111705\n",
      "Epoch [36/80], Step [1/1], Loss: 1.0073103\n",
      "Epoch [37/80], Step [1/1], Loss: 1.0084339\n",
      "Epoch [38/80], Step [1/1], Loss: 1.0087844\n",
      "Epoch [39/80], Step [1/1], Loss: 1.0097964\n",
      "Epoch [40/80], Step [1/1], Loss: 1.0079776\n",
      "Epoch [41/80], Step [1/1], Loss: 1.0064023\n",
      "Epoch [42/80], Step [1/1], Loss: 1.0036280\n",
      "Epoch [43/80], Step [1/1], Loss: 1.0042742\n",
      "Epoch [44/80], Step [1/1], Loss: 1.0032807\n",
      "Epoch [45/80], Step [1/1], Loss: 1.0033947\n",
      "Epoch [46/80], Step [1/1], Loss: 1.0048315\n",
      "Epoch [47/80], Step [1/1], Loss: 1.0061349\n",
      "Epoch [48/80], Step [1/1], Loss: 1.0059906\n",
      "Epoch [49/80], Step [1/1], Loss: 1.0105472\n",
      "Epoch [50/80], Step [1/1], Loss: 1.0058069\n",
      "Epoch [51/80], Step [1/1], Loss: 1.0066661\n",
      "Epoch [52/80], Step [1/1], Loss: 1.0041668\n",
      "Epoch [53/80], Step [1/1], Loss: 1.0030083\n",
      "Epoch [54/80], Step [1/1], Loss: 1.0024545\n",
      "Epoch [55/80], Step [1/1], Loss: 1.0009879\n",
      "Epoch [56/80], Step [1/1], Loss: 1.0017268\n",
      "Epoch [57/80], Step [1/1], Loss: 0.9995654\n",
      "Epoch [58/80], Step [1/1], Loss: 0.9999402\n",
      "Epoch [59/80], Step [1/1], Loss: 0.9992379\n",
      "Epoch [60/80], Step [1/1], Loss: 0.9983709\n",
      "Epoch [61/80], Step [1/1], Loss: 1.0018448\n",
      "Epoch [62/80], Step [1/1], Loss: 1.0004529\n",
      "Epoch [63/80], Step [1/1], Loss: 1.0002108\n",
      "Epoch [64/80], Step [1/1], Loss: 0.9992422\n",
      "Epoch [65/80], Step [1/1], Loss: 0.9979727\n",
      "Epoch [66/80], Step [1/1], Loss: 0.9963875\n",
      "Epoch [67/80], Step [1/1], Loss: 0.9973910\n",
      "Epoch [68/80], Step [1/1], Loss: 0.9968347\n",
      "Epoch [69/80], Step [1/1], Loss: 0.9959781\n",
      "Epoch [70/80], Step [1/1], Loss: 0.9958426\n",
      "Epoch [71/80], Step [1/1], Loss: 0.9959192\n",
      "Epoch [72/80], Step [1/1], Loss: 0.9965194\n",
      "Epoch [73/80], Step [1/1], Loss: 0.9958742\n",
      "Epoch [74/80], Step [1/1], Loss: 0.9951059\n",
      "Epoch [75/80], Step [1/1], Loss: 0.9947437\n",
      "Epoch [76/80], Step [1/1], Loss: 0.9943882\n",
      "Epoch [77/80], Step [1/1], Loss: 0.9942989\n",
      "Epoch [78/80], Step [1/1], Loss: 0.9940379\n",
      "Epoch [79/80], Step [1/1], Loss: 0.9937641\n",
      "Epoch [80/80], Step [1/1], Loss: 0.9935539\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss(reduction = 'sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device).float()\n",
    "        #print(X.shape)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 1 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}], Loss: {:.7f}\"\n",
    "                  .format(epoch+1, num_epochs, i+1, total_step, loss.item()/330))\n",
    "            \n",
    "    # Decay learning rate\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        curr_lr /= 1\n",
    "        update_lr(optimizer, curr_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 2.4539877300613497 %, Loss: 1.049238920211792\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "        outputs = model(images)\n",
    "        loss += criterion(outputs, labels.float()) \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.shape[0]\n",
    "        labels_new = torch.max(labels, 1)[1]\n",
    "        correct += (predicted == labels_new).sum().item()\n",
    "        \n",
    "    print('Accuracy of the model on the test images: {} %, Loss: {}'\n",
    "          .format(100 * correct / total, loss/163))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'resnet.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "dummy_input = torch.rand((100,30,64,64))\n",
    "with SummaryWriter(comment = 'ResNet') as writer:\n",
    "    writer.add_graph(model,(X, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 64,  64,  64, 330,  64,  64,  51,  64,  64,  64,  96,  64,  64,  64,\n",
       "         64,  64,  64,  64,  64,  64,  64,  64,  64, 427,  64,  64,  64,  64,\n",
       "         64,  64,  64,  64,  64,  64,  64,  64,  64,  64,  64,  64,  64,  64,\n",
       "         64,  64,  64,  64,  64,  64,  64,  51,  64,  64,  64,  64,  64,  64,\n",
       "         64,  64,  64,  64,  64,  64,  64], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
