{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydicom\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import random\n",
    "from matplotlib import image\n",
    "from scipy.ndimage import label\n",
    "from zipfile import ZipFile\n",
    "import re\n",
    "import pandas as pd\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this varibale should be set as where your train.zip, validate.zip, test.zip store\n",
    "data_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ZipFile Object and load train.zip in it\n",
    "#with ZipFile(os.path.join(data_path, \"train.zip\"), 'r') as zipObj:\n",
    "#   # Extract all the contents of zip file in different directory\n",
    "#   zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_train = next(os.walk(os.path.join(data_path, \"train\")))[1]\n",
    "# load labels in 'train.csv'\n",
    "# the first column means id\n",
    "# the second and third columns mean the volume\n",
    "labels = np.loadtxt(os.path.join(data_path, \"train.csv\"), delimiter=\",\",skiprows=1)\n",
    "labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    dataset_count = 0\n",
    "\n",
    "    def __init__(self, directory, subdir):\n",
    "        # deal with any intervening directories\n",
    "        while True:\n",
    "            subdirs = next(os.walk(directory))[1]\n",
    "            if len(subdirs) == 1:\n",
    "                directory = os.path.join(directory, subdirs[0])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        slices = []\n",
    "        for s in subdirs:\n",
    "            m = re.match(\"sax_(\\d+)\", s)\n",
    "            if m is not None:\n",
    "                slices.append(int(m.group(1)))\n",
    "\n",
    "        slices_map = {}\n",
    "        first = True\n",
    "        times = []\n",
    "        for s in slices:\n",
    "            files = next(os.walk(os.path.join(directory, \"sax_%d\" % s)))[2]\n",
    "            offset = None\n",
    "\n",
    "            for f in files:\n",
    "                m = re.match(\"IM-(\\d{4,})-(\\d{4})\\.dcm\", f)\n",
    "                if m is not None:\n",
    "                    if first:\n",
    "                        times.append(int(m.group(2)))\n",
    "                    if offset is None:\n",
    "                        offset = int(m.group(1))\n",
    "\n",
    "            first = False\n",
    "            slices_map[s] = offset\n",
    "\n",
    "        self.directory = directory\n",
    "        self.time = sorted(times)\n",
    "        self.slices = sorted(slices)\n",
    "        self.slices_map = slices_map\n",
    "        Dataset.dataset_count += 1\n",
    "        self.name = subdir\n",
    "\n",
    "    def _filename(self, s, t):\n",
    "        return os.path.join(self.directory,\"sax_%d\" % s, \"IM-%04d-%04d.dcm\" % (self.slices_map[s], t))\n",
    "\n",
    "    def _read_dicom_image(self, filename):\n",
    "        d = pydicom.read_file(filename)\n",
    "        img = d.pixel_array\n",
    "        IMG_PX_SIZE = 64\n",
    "        resized_img = resize(img, (IMG_PX_SIZE, IMG_PX_SIZE), anti_aliasing=True)\n",
    "        return np.array(resized_img)\n",
    "\n",
    "    def _read_all_dicom_images(self):\n",
    "        f1 = self._filename(self.slices[0], self.time[0])\n",
    "        d1 = pydicom.read_file(f1)\n",
    "        (x, y) = d1.PixelSpacing\n",
    "        (x, y) = (float(x), float(y))\n",
    "        f2 = self._filename(self.slices[1], self.time[0])\n",
    "        d2 = pydicom.read_file(f2)\n",
    "\n",
    "        # try a couple of things to measure distance between slices\n",
    "        try:\n",
    "            dist = np.abs(d2.SliceLocation - d1.SliceLocation)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                dist = d1.SliceThickness\n",
    "            except AttributeError:\n",
    "                dist = 8  # better than nothing...\n",
    "\n",
    "        self.images = np.array([[self._read_dicom_image(self._filename(d, i))\n",
    "                                 for i in self.time]\n",
    "                                for d in self.slices])\n",
    "        self.dist = dist\n",
    "        self.area_multiplier = x * y\n",
    "\n",
    "    def load(self):\n",
    "        self._read_all_dicom_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = []\n",
    "for i,s in enumerate(study_train):\n",
    "    full_path = os.path.join(data_path, \"train\", s)\n",
    "    dset.append(Dataset(full_path, s))\n",
    "    print(\"Processing dataset %s...\" % dset[i].name)\n",
    "    p_edv = 0\n",
    "    p_esv = 0\n",
    "    try:\n",
    "        dset[i].load()\n",
    "        print(\"Dataset %s processing done.\" % dset[i].name)\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: Exception %s thrown by dataset %s\" % (str(e), dset[i].name))\n",
    "        print(\"Omit index: %s\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note 337, 437, 463, 499, 234, 393, 334, 305, 279, 416, 41, 123 can not loaded \n",
    "# so we just remove them from our training data\n",
    "omit_subject = [463, 499, 234, 334, 279, 416, 123]\n",
    "omit_index = [134, 140, 169, 305, 333, 352, 433]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refine the whole dataset\n",
    "study_index = [int(ele) for ele in study_train]\n",
    "study_index = [study_index[i] for i in range(len(study_index)) if i not in omit_index]\n",
    "X = []\n",
    "for ind, val in enumerate(study_train):\n",
    "    try:\n",
    "        new_image = dset[ind].images\n",
    "        X.append(new_image)\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: Exception %s\" % str(e))\n",
    "        print(\"Stop at index: %s\" % val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[3].swapaxes(1,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For example, to simplify this problem, we may just take the average of images\n",
    "# for each subject\n",
    "# Of course, you can consider more complicated method to obtain better performance\n",
    "X_train_average = []\n",
    "for i in range(len(X)):\n",
    "    images = X[i].swapaxes(1,3)\n",
    "    t, s, w, h = X[i].shape\n",
    "    image_sum = np.zeros([64,64,30])\n",
    "    for j in range(t):\n",
    "        image_sum = image_sum + images[j,:,:,:]\n",
    "            \n",
    "    image_average = image_sum / t\n",
    "    X_train_average.append(image_average)\n",
    "X_train_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target for example: systole only\n",
    "label = pd.DataFrame(labels)\n",
    "label.columns = ['Id','Systole','Diastole']\n",
    "actual_value = np.round(label.loc[np.array(study_index)-1,'Systole']).astype(int)\n",
    "Y_train = actual_value\n",
    "\n",
    "# one-hot encode\n",
    "label_train = np.zeros([len(Y_train), 600])\n",
    "for i in range(len(Y_train)):\n",
    "    value = Y_train.iloc[i]\n",
    "    label_train[i,value-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor\n",
    "X_train_average = np.array(X_train_average).reshape([-1, 64, 64, 30])\n",
    "X_train_average.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_average.reshape([493,64*64])\n",
    "#X_train_average[0].reshape([64*64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_average, label_train, test_size=0.33, random_state=42)\n",
    "#np.savetxt('X_train.csv', X_train.reshape((330,64*64*30)), delimiter=',')\n",
    "#np.savetxt('X_test.csv', X_test.reshape((163,64*64*30)), delimiter=',')\n",
    "#np.savetxt('y_train.csv',y_train, delimiter=',')\n",
    "#np.savetxt('y_test.csv', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('X_train.npy', X_train)\n",
    "X_train_load = np.load('X_train.npy')\n",
    "\n",
    "#np.save('y_train.npy', y_train)\n",
    "y_train_load = np.load('y_train.npy')\n",
    "\n",
    "#np.save('X_test.npy', X_test)\n",
    "X_test_load = np.load('X_test.npy')\n",
    "\n",
    "#np.save('y_test.npy', y_test)\n",
    "y_test_load = np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_load = np.loadtxt('X_train.csv', delimiter = ',').reshape(-1,64,64,30)\n",
    "#X_test = np.loadtxt('X_test.csv', delimiter = ',').reshape(-1,64,64,1)\n",
    "#y_train = np.loadtxt('y_train.csv', delimiter = ',')\n",
    "#y_test = np.loadtxt('y_test.csv', delimiter = ',')\n",
    "#assert X_train_load == X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tflearn\\initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tflearn\\layers\\core.py:239: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "network = input_data(shape=[None, 64, 64, 30], name='input')\n",
    "network = conv_2d(network, 32, 4, activation='relu')\n",
    "network = max_pool_2d(network, 2)\n",
    "network = conv_2d(network, 64, 4, activation='relu')\n",
    "network = max_pool_2d(network, 2)\n",
    "network = fully_connected(network, 128, activation='relu')\n",
    "network = dropout(network, 0.8)\n",
    "network = fully_connected(network, 256, activation='relu')\n",
    "network = dropout(network, 0.8)\n",
    "network = fully_connected(network, 600, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "network = regression(network, optimizer='adam', learning_rate=0.01,\n",
    "                     loss='categorical_crossentropy', name='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m4.61479\u001b[0m\u001b[0m | time: 0.193s\n",
      "| Adam | epoch: 010 | loss: 4.61479 - acc: 0.0527 -- iter: 320/330\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m4.62947\u001b[0m\u001b[0m | time: 1.252s\n",
      "| Adam | epoch: 010 | loss: 4.62947 - acc: 0.0498 | val_loss: 5.47604 - val_acc: 0.0184 -- iter: 330/330\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "model.fit({'input': X_train_load}, {'target': y_train_load}, n_epoch=10,\n",
    "           validation_set=({'input': X_test_load}, {'target': y_test_load}),\n",
    "           snapshot_step=100, show_metric=True, run_id='convnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict({'input': X_test_load})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:,1]\n",
    "#np.savetxt('y_result.csv',pred, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
